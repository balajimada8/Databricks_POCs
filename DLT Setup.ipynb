{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513d376d-bb81-4a1a-9d46-97913bf31e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import dlt\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import regexp_replace, to_date, col, trim, count, sha2, concat_ws\n",
    "\n",
    "\n",
    "\n",
    "# Bronze Layer\n",
    "@dlt.table(\n",
    "    comment=\"Raw Medium post data ingested from CSV\"\n",
    ")\n",
    "@dlt.expect(\"No null links\", \"link IS NOT NULL\")\n",
    "def medium_raw():\n",
    "    csv_path = \"/FileStore/tables/fe_medium_posts_raw.csv\"\n",
    "    return spark.read.csv(csv_path, header=True)\n",
    "\n",
    "\n",
    "# Silver Layer - Clean and deduplicate\n",
    "@dlt.table(\n",
    "    comment=\"Cleaned and deduplicated Medium posts\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"Valid publish date\", \"published_on IS NOT NULL AND published_on != ''\")\n",
    "def medium_clean():\n",
    "    df = dlt.read(\"medium_raw\")\n",
    "    df = df.filter(df.link.isNotNull() & (df.link != 'null'))\n",
    "\n",
    "    # Clean author name\n",
    "    df = df.withColumn(\"author\", trim(regexp_replace(col(\"author\"), r\"\\([^()]*\\)\", \"\")))\n",
    "\n",
    "    # Normalize publish date\n",
    "    df = df.withColumn(\"publish_date\", to_date(\"published_on\"))\n",
    "\n",
    "    \"\"\" Generate a unique hash for deduplication\n",
    "    df = df.withColumn(\"row_hash\", sha2(concat_ws(\"||\", *df.columns), 256))\n",
    "    df = df.dropDuplicates([\"row_hash\"])\"\"\"\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Silver Layer - Recent posts view\n",
    "@dlt.view\n",
    "def recent_posts():\n",
    "    return dlt.read(\"medium_clean\").filter(col(\"publish_date\") >= \"2023-01-01\")\n",
    "\n",
    "\n",
    "# Gold Layer - Post summary per author\n",
    "@dlt.table(\n",
    "    comment=\"Aggregated number of recent posts per author\"\n",
    ")\n",
    "def author_post_counts():\n",
    "    return (\n",
    "        dlt.read(\"recent_posts\")\n",
    "        .groupBy(\"author\")\n",
    "        .agg(count(\"*\").alias(\"post_count\"))\n",
    "        .orderBy(col(\"post_count\").desc())\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT Setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
